{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adata\n",
    "\n",
    "> bite sized functions for processing adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# NOTE: needed for python 3.10 forward compatibility with scanpy as \n",
    "# scanpy uses Iterable which is deprecated in 3.10\n",
    "import collections.abc\n",
    "#hyper needs the four following aliases to be done manually.\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "collections.Mapping = collections.abc.Mapping\n",
    "collections.MutableSet = collections.abc.MutableSet\n",
    "collections.MutableMapping = collections.abc.MutableMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "\n",
    "import numpy as np, pandas as pd, scipy\n",
    "import phate, magic, graphtools as gt\n",
    "import scprep, anndata as ad, scanpy as sc, scrublet as scr\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from typing import TypeAlias, List, Sequence, Tuple, Optional, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from degex.types import (\n",
    "    AnnData, AnnDatas, Graph, SeriesLike,\n",
    "    CutoffSpec, CutoffSpecs, CUTOFF,\n",
    ")\n",
    "#| export\n",
    "from degex.utils import (\n",
    "    arr_toarray, adata_X_toarray, \n",
    "    time_to_num_from_idx_to_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from degex.static import (\n",
    "    SEED,\n",
    "    GENE_IDS, GENE_SYMBOL, HUMAN_GENE_SYMBOL, HIGHLY_VARIABLE,\n",
    "    ENSEMBL, HUMAN_TF, MOUSE_TF, HUMAN_ENSEMBLE_ID, MOUSE_ENSEMBLE_ID,\n",
    "    TOTAL_COUNTS, BATCH, MITO, RIBO, TIMEPOINT,\n",
    "    DOUBLET_SCORES, PREDICTED_DOUBLETS,\n",
    "    ADATA, PCA, PHATE,\n",
    "    X_MAGIC, X_PCA, X_PCA_HVG, X_PHATE, X_PHATE_HVG,\n",
    "    X_PRENORM, X_DETECTED, X_SCALED_NORMALIZED,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust AnnData `var`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def set_gene_symbol_as_var_names(adata: AnnData) -> AnnData:\n",
    "    \"\"\"\n",
    "    Enfoces that `adata.var` names are unique and adds \n",
    "    `gene_symbol` to `adata.var`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    \"\"\"\n",
    "    adata.var_names_make_unique()\n",
    "    if GENE_SYMBOL not in adata.var:\n",
    "        adata.var[GENE_SYMBOL] = adata.var_names\n",
    "    return adata\n",
    "\n",
    "def set_var_names_as_gene_ids(adata: AnnData) -> AnnData:\n",
    "    \"\"\"\n",
    "    Adds `gene_ids` to `adata.var`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    \"\"\"\n",
    "    adata.var_names = adata.var[GENE_IDS]\n",
    "    return adata\n",
    "\n",
    "def remove_mitochondrial_genes(adata: AnnData) -> AnnData:\n",
    "    \"\"\"\n",
    "    Filters `adata` by colums not in `adata.var.mito`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    \"\"\"\n",
    "    adata = adata[:, ~(adata.var[MITO])]\n",
    "    return adata\n",
    "\n",
    "def score_doublets(adata: AnnData, plot: bool = False, **kwargs) -> AnnData:   \n",
    "    f\"\"\"\n",
    "    Adds `{DOUBLET_SCORES}` and `{PREDICTED_DOUBLETS}` to `adata.obs`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process. Note that `adata.X` gets passed \n",
    "        as `counts_matrix` to `scrublet.Scrublet(...)`.\n",
    "\n",
    "    plot\n",
    "        Whether or not to plot scrublet histogram.\n",
    "\n",
    "    kwargs\n",
    "        Other key-value arguments to passed to `scrublet.Scrublet()`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    \"\"\"\n",
    "    assert 'counts_matrix' not in kwargs\n",
    "    scrub = scr.Scrublet(counts_matrix=adata.X, **kwargs)\n",
    "    adata.obs[DOUBLET_SCORES], adata.obs[PREDICTED_DOUBLETS] =\\\n",
    "        scrub.scrub_doublets()\n",
    "    if plot:\n",
    "        scrub.plot_histogram()\n",
    "    return adata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_gene_annotations(\n",
    "    adata: AnnData,\n",
    "    annotation_file: str\n",
    ") -> AnnData:\n",
    "    \"\"\"\n",
    "    Reads CSV and adds Ensembl information to \n",
    "    `adata.var.index`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process\n",
    "\n",
    "    annotation_file\n",
    "        full path to csv file. Assumed to have a column\n",
    "        called `Ensembl`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Assumes that `annotation_file` has a column `'Ensembl'` in it.\n",
    "    \"\"\"\n",
    "    # Load gene annotation information (extracted from bioconductor)    \n",
    "    gene_annotation = pd.read_csv(\n",
    "        annotation_file, index_col=None, header=0\n",
    "    ).astype(str)\n",
    "\n",
    "    assert hasattr(gene_annotation, ENSEMBL.capitalize())\n",
    "    gene_annotation.index = list(gene_annotation.Ensembl)\n",
    "\n",
    "    # Add to AnnData object\n",
    "    adata.var = pd.concat(\n",
    "        [adata.var, gene_annotation], axis=1, join='inner'\n",
    "    )\n",
    "    adata.var.index = list(adata.var[GENE_SYMBOL])\n",
    "    # Enforce uniqueness\n",
    "    adata.var_names_make_unique()\n",
    "    return adata\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack and Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stack(\n",
    "    *adatas: AnnDatas,\n",
    "    key: str = '_idx',    \n",
    "    replace: Optional[Dict[str,str]] = None,\n",
    "    replace_key: Optional[str] = None,\n",
    "    print_counts: bool = False\n",
    ") -> AnnData:            \n",
    "    adata = ad.concat([*adatas], index_unique=\"_\", merge=\"same\", join='outer')\n",
    "\n",
    "    adata.obs[key] = adata.obs.index.astype(str).str[-1]\n",
    "    if replace is not None:\n",
    "        rename_key = key if replace_key is None else replace_key\n",
    "        adata.obs[rename_key] = adata.obs[key].replace(replace)\n",
    "\n",
    "    if print_counts:\n",
    "        print(adata.obs[key].value_counts())\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stack_batchs(\n",
    "    *adatas: AnnDatas, \n",
    "    idx_to_time: dict,\n",
    "    idx_to_batch: dict = None,\n",
    "    batch_to_timepoint: dict = None,\n",
    "    print_counts: bool = False\n",
    ") -> AnnData:    \n",
    "    \"\"\"\n",
    "    Concatenates the each one of the `adata`s in  \n",
    "    `adatas` to a single `adata` instance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    *adatas\n",
    "        AnnData to process. Note that `adata.X` gets passed \n",
    "        as `counts_matrix` to `scrublet.Scrublet(...)`.\n",
    "\n",
    "    idx_to_time\n",
    "        map of {int|str: str} for indicies to human friendly \n",
    "        time values.\n",
    "\n",
    "    idx_to_batch\n",
    "        map of {int|str: str} for indicies to batch\n",
    "\n",
    "    batch_to_timepoint\n",
    "        map of {str: int} to map each batch (from `idx_to_time` or `idx_to_batch`) to a timepoint\n",
    "\n",
    "    print_counts\n",
    "        Whether or not to print batched value counts.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        Concatenated adata.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>>    idx_to_time = {\n",
    "            '0': '12hr', \n",
    "            '1': '18hr', \n",
    "            '2': '24hr'\n",
    "        }\n",
    "\n",
    "    >>>   time_to_num = {\n",
    "            '12hr': '12', \n",
    "            '18hr': '18', \n",
    "            '24hr': '24'\n",
    "        }\n",
    "    \"\"\"\n",
    "    time_to_num = time_to_num_from_idx_to_time(idx_to_time)\n",
    "\n",
    "    replace_batch = idx_to_time if idx_to_batch is None else idx_to_batch\n",
    "    replace_times = time_to_num if batch_to_timepoint is None else batch_to_timepoint\n",
    "\n",
    "    adata = stack(*adatas, key=BATCH, replace=replace_batch)\n",
    "    adata.obs[TIMEPOINT] = adata.obs[BATCH].replace(replace_times)\n",
    "    \n",
    "    if print_counts:\n",
    "        print(adata.obs[BATCH].value_counts())\n",
    "    return adata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "QC_VARS = dict()\n",
    "QC_VARS[MITO] = ('mt-', )\n",
    "QC_VARS[RIBO] = ('rps', 'rpl')\n",
    "\n",
    "def var_starts_with_pattern(name:str, patterns: Optional[Sequence[str]] = None) -> tuple:\n",
    "    if patterns is None:\n",
    "        patterns = (f'{name.lower()}_', )\n",
    "    return patterns\n",
    "\n",
    "def make_var_starts_with(var_starts_with: Optional[Dict[str, tuple]] = dict()):\n",
    "    return {k: var_starts_with_pattern(k, v) for k, v in var_starts_with.items()}\n",
    "\n",
    "\n",
    "def calc_qc_stats(\n",
    "    adata: AnnData,\n",
    "    qc_vars: Dict[str, tuple] = QC_VARS,\n",
    ") -> AnnData:\n",
    "    f\"\"\"\n",
    "    Calculates {MITO} and {RIBO} qc metrics\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process\n",
    "\n",
    "    annotation_file\n",
    "        full path to csv file. Assumed to have a column\n",
    "        called `Ensembl`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    \"\"\"\n",
    "    qc_vars = make_var_starts_with(qc_vars)\n",
    "    # Calculate QC stats\n",
    "    for vname, patterns in qc_vars.items():\n",
    "        adata.var[vname] = adata.var_names.str.startswith(patterns)\n",
    "\n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars = list(qc_vars.keys()), inplace = True)\n",
    "\n",
    "    adata.obs['log10_total_counts'] = np.log10(adata.obs[TOTAL_COUNTS])\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def filter_by_cutoffs(\n",
    "    adata: AnnData, \n",
    "    lower: float = None, \n",
    "    upper: float = None,\n",
    "    obs_key: CUTOFF = TOTAL_COUNTS,\n",
    "    print_counts: bool = False,      \n",
    ") -> AnnData:\n",
    "    \"\"\"\n",
    "    Uses `obs_key` to filter `adata` between `lower` and `upper` if provided e.g. \n",
    "    `lower < adata.obs[obs_key] < upper`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process.\n",
    "\n",
    "    lower\n",
    "        Defaults to `None`. The value which `adata.obs[obs_key]` \n",
    "        should be greater than. If `None`, `lower` is not used.\n",
    "    \n",
    "    upper\n",
    "        Defaults to `None`. The value which `adata.obs[obs_key]` \n",
    "        should be less than. If `None`, `upper` is not used.\n",
    "\n",
    "    obs_key\n",
    "        Which observation to test against.\n",
    "        One of `'total_counts'`, `'pct_counts_mito'`, \n",
    "        `'pct_counts_ribo'`, or `'doublet_score'`. Defaults\n",
    "        to `'total_counts'`. \n",
    "\n",
    "    print_counts\n",
    "        Whether or not to print counts.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    \"\"\"\n",
    "    assert obs_key is not None    \n",
    "    if lower is not None:\n",
    "        adata = adata[adata.obs[obs_key] > lower]\n",
    "\n",
    "    if upper is not None:\n",
    "        adata = adata[adata.obs[obs_key] < upper]\n",
    "\n",
    "    if print_counts:\n",
    "        print(adata.obs.batch.value_counts())\n",
    "    return adata\n",
    "\n",
    "\n",
    "def apply_filter_by_cutoffs(\n",
    "    adata: AnnData, \n",
    "    cutoff_specs: CutoffSpecs,\n",
    "    print_counts: bool = False   \n",
    ") -> AnnData:\n",
    "    \"\"\"\n",
    "    Uses `obs_key` to filter `adata` between `lower` and `upper` if provided e.g. \n",
    "    `lower < adata.obs[obs_key] < upper`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process.\n",
    "\n",
    "    cutoff_specs\n",
    "        Specifications in the form of `(obs_key, lower, upper)`\n",
    "        to be used to `filter_by_cutoffs`.\n",
    "\n",
    "    print_counts\n",
    "        Whether or not to print counts.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    filter_by_cutoffs: singular instance of filtering.\n",
    "    \"\"\"\n",
    "    for spec in cutoff_specs:\n",
    "        adata = filter_by_cutoffs(\n",
    "            adata, spec.lower, spec.upper, \n",
    "            spec.obs_key, print_counts\n",
    "        )\n",
    "    return adata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_prenormalization_layer(adata: AnnData) -> AnnData:\n",
    "    f\"\"\"\n",
    "    Stores `adata.X` to `layers[{X_PRENORM}]`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    \"\"\"\n",
    "    # Store unnormalised counts\n",
    "    adata.layers[X_PRENORM] = adata.X\n",
    "    return adata\n",
    "\n",
    "def add_gene_detection_layer(adata: AnnData) -> AnnData:\n",
    "    f\"\"\"\n",
    "    Stores `adata.X > 0` to `layers[{X_DETECTED}]`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    \"\"\"\n",
    "    # Store unnormalised counts\n",
    "    if X_PRENORM not in adata.layers:\n",
    "        adata = add_prenormalization_layer(adata)\n",
    "\n",
    "    # Add layer of gene detection\n",
    "    adata.layers[X_DETECTED] = scipy.sparse.csr_matrix(\n",
    "        pd.DataFrame(\n",
    "        (arr_toarray(adata.layers[X_PRENORM]) > 0), \n",
    "        columns = adata.var.index, index=adata.obs.index\n",
    "    ).replace({True: 1, False: 0}))\n",
    "    return adata\n",
    "\n",
    "\n",
    "def sqrt_library_size_normalize(adata: AnnData) -> AnnData:\n",
    "    f\"\"\"\n",
    "    Runs `sqrt(library_size_normalize(adata.X))` and stores\n",
    "    it in `adata.X`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    \"\"\"\n",
    "    # Normalise by library size and square-root transform\n",
    "    adata = adata.copy()\n",
    "    adata.X = scipy.sparse.csr_matrix(\n",
    "        scprep.transform.sqrt(\n",
    "            scprep.normalize.library_size_normalize(\n",
    "                adata_X_toarray(adata)                \n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_batch_mean_center_layer(adata: AnnData) -> AnnData:\n",
    "    f\"\"\"\n",
    "    Runs `batch_mean_center(adata.X)` and stores\n",
    "    it in `adata.layers[{X_SCALED_NORMALIZED}]`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    \"\"\"\n",
    "    # Batch mean center before cell cycle scoring\n",
    "    adata.raw = adata\n",
    "    adata.X = scipy.sparse.csr_matrix(\n",
    "        scprep.normalize.batch_mean_center(        \n",
    "            adata_X_toarray(adata),            \n",
    "            sample_idx = adata.obs[BATCH]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    adata.layers[X_SCALED_NORMALIZED] = scipy.sparse.csr_matrix(adata.X)\n",
    "    adata.X = adata.raw.X\n",
    "    return adata\n",
    "\n",
    "def score_genes_cell_cycle_with_batch_mean_center_data(\n",
    "        adata: AnnData,\n",
    "        s_genes:Sequence[str], \n",
    "        g2m_genes:Sequence[str],\n",
    ") -> AnnData:\n",
    "    f\"\"\"\n",
    "    Uses `adata.layers[{X_SCALED_NORMALIZED}]` to run\n",
    "    `sc.tl.score_genes_cell_cycle` and stores results in \n",
    "    `adata`.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process.\n",
    "\n",
    "    s_genes\n",
    "        List of genes associated with S phase.\n",
    "\n",
    "    g2m_genes\n",
    "        List of genes associated with G2M phase.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    \"\"\"\n",
    "    sdata = adata.copy()\n",
    "    sdata.X = np.array(adata.layers[X_SCALED_NORMALIZED].todense())        \n",
    "    sdata.raw = adata\n",
    "    # Get normalised counts back instead of mean centered values as pca will mean center\n",
    "    sc.tl.score_genes_cell_cycle(sdata, s_genes=s_genes, g2m_genes=g2m_genes)\n",
    "\n",
    "    adata.obs = adata.obs.join(sdata.obs.S_score)\n",
    "    adata.obs = adata.obs.join(sdata.obs.G2M_score)\n",
    "    adata.obs = adata.obs.join(sdata.obs.phase)\n",
    "\n",
    "    return adata\n",
    "\n",
    "def load_human_genes(\n",
    "    adata: AnnData, filename: str\n",
    ") -> List[str]:\n",
    "    f'''\n",
    "    Reads the file uses `adata` to confirm validity\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process.\n",
    "\n",
    "    filename\n",
    "        Plaintext file with one column and a single gene\n",
    "        on each row with in its HumanGeneSymbol form.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Assumes `adata.var` has `{HUMAN_GENE_SYMBOL}`. \n",
    "    '''\n",
    "    assert hasattr(adata.var, HUMAN_GENE_SYMBOL)\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        genes = f.readlines()\n",
    "        genes = [gene.strip() for gene in genes]\n",
    "        genes = adata.var.index[adata.var[HUMAN_GENE_SYMBOL].isin(genes)]\n",
    "        return genes\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highly Variable Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def select_hvg_per_batch(\n",
    "    adata: AnnData,\n",
    "    hvg_kwargs: dict = dict(cutoff=None, percentile=90)\n",
    ") -> AnnData:\n",
    "    '''\n",
    "    Calculates highly variable genes per batch in `adata`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData to process.\n",
    "\n",
    "    hvg_kwargs\n",
    "        Options to be passed to `sc.select_highly_variable_genes`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata\n",
    "        for function chaining\n",
    "    '''\n",
    "    # Select highly variable genes from any batch\n",
    "    hvg_all = []\n",
    "    for batch in adata.obs[BATCH].unique():\n",
    "        normalised, hgv_vars = scprep.select.highly_variable_genes(\n",
    "            adata_X_toarray(adata[adata.obs[BATCH] == batch]),\n",
    "            adata[adata.obs[BATCH] == batch].var.index, \n",
    "            **hvg_kwargs\n",
    "        )\n",
    "        hvg_all.extend(hgv_vars)\n",
    "        adata.var[f'{HIGHLY_VARIABLE}_{batch}'] = adata.var.index.isin(hgv_vars)\n",
    "        del normalised\n",
    "        print(f\"Unique HVGs after {batch} {len(np.unique(np.array(hvg_all)))}\")\n",
    "        \n",
    "    adata.var[HIGHLY_VARIABLE] = adata.var.index.isin(hvg_all)\n",
    "    return adata\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def add_tf_annotations_from_csv(\n",
    "    adata: AnnData, filename: str,\n",
    "    tf_key: str, ensemble_key: str,\n",
    "    print_counts: bool = False\n",
    ") -> AnnData:\n",
    "    assert hasattr(adata.var, ensemble_key)\n",
    "    df_tfs = pd.read_csv(filename, index_col=None, header=0).astype(str)\n",
    "    \n",
    "    adata.var[tf_key] = adata.var[ensemble_key]\\\n",
    "        .isin(df_tfs[ensemble_key])\n",
    "\n",
    "    if print_counts:\n",
    "        print(adata.var[tf_key].value_counts())\n",
    "    return adata\n",
    "\n",
    "def add_human_tfs_from_csv(\n",
    "    adata: AnnData, filename:str,\n",
    "    print_counts:bool=False\n",
    ") -> AnnData:\n",
    "    return add_tf_annotations_from_csv(\n",
    "        adata, filename, HUMAN_TF,\n",
    "        HUMAN_ENSEMBLE_ID, print_counts\n",
    "    )\n",
    "\n",
    "def add_mouse_tfs_from_csv(\n",
    "    adata: AnnData, filename:str,\n",
    "    print_counts:bool=False\n",
    ") -> AnnData:    \n",
    "    return add_tf_annotations_from_csv(\n",
    "        adata, filename, MOUSE_TF,\n",
    "        MOUSE_ENSEMBLE_ID, print_counts\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def zscore_markers_in_layer(\n",
    "    adata: AnnData,\n",
    "    markers: List[str],\n",
    "    obs_key: str = 'Markers_zscore',\n",
    "    layer_key: str = X_MAGIC,    \n",
    ") -> AnnData:    \n",
    "    # Score cells based on select marker expression (sum of zscores of smoothed counts)\n",
    "    col_subset = adata.var.index.isin(markers)\n",
    "    df_markers = pd.DataFrame(\n",
    "        arr_toarray(adata.layers[layer_key][:, col_subset]),        \n",
    "        columns = adata.var.index[col_subset],\n",
    "        index = adata.obs.index\n",
    "    )\n",
    "    df_markers.apply(zscore)\n",
    "    adata.obs[obs_key] = df_markers.sum(axis=1)\n",
    "    return adata\n",
    "\n",
    "def subset_markers(\n",
    "    adata: AnnData,\n",
    "    obs_key: str = 'Markers_cell',\n",
    "    score_key: str = 'Markers_zscore',\n",
    "    lower: float = 2.2,\n",
    "    upper: float = None,\n",
    "    marker_name: str = 'marker',\n",
    "    other_name: str = 'other'\n",
    ") -> AnnData:\n",
    "    u_cut = pd.Series(np.repeat(True, adata.obs.shape[0]))\n",
    "    l_cut = pd.Series(np.repeat(True, adata.obs.shape[0]))\n",
    "    if upper is not None:\n",
    "        u_cut = (adata.obs[score_key] < upper)\n",
    "\n",
    "    if lower is not None:\n",
    "        l_cut = (lower < adata.obs[score_key])\n",
    "    \n",
    "    found = pd.Series(np.logical_and(u_cut.values, l_cut.values), index=adata.obs[score_key].index, name=obs_key)\n",
    "\n",
    "    adata.obs[obs_key] = (found).replace({True: marker_name, False: other_name})\n",
    "    return adata\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "PCA_KWARGS: dict = dict(n_components=100)\n",
    "PHATE_KWARGS: dict = dict(n_components=3, t=70)\n",
    "G_KWARGS: dict = dict(knn=10)\n",
    "MAGIC_KWARGS: dict = dict(solver='approximate', n_jobs=-1, knn_max=60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_pca(\n",
    "    adata: AnnData,\n",
    "    pca_kwargs: dict = PCA_KWARGS,\n",
    "    plot_scree: bool = False,\n",
    "    emb_key: str = X_PCA,\n",
    "    col_subset: SeriesLike = None\n",
    ") -> AnnData:\n",
    "    # Compute PCs for initial cell graph\n",
    "    pca_kwargs['return_singular_values'] = True\n",
    "    pca_kwargs[SEED] = 3\n",
    "\n",
    "    if col_subset is not None:        \n",
    "        x = adata_X_toarray(adata[:, col_subset])\n",
    "    else:\n",
    "        x = adata_X_toarray(adata)        \n",
    "\n",
    "    pcs, svs = scprep.reduce.pca(x, **pca_kwargs)\n",
    "    adata.obsm[emb_key] = pcs\n",
    "    if plot_scree:\n",
    "        scprep.plot.scree_plot(svs, cumulative=False)\n",
    "    return adata\n",
    "\n",
    "def run_pca_on_hvg(\n",
    "    adata: AnnData, pca_kwargs: dict = PCA_KWARGS, plot_scree: bool = False,\n",
    ") -> AnnData:\n",
    "    return run_pca(\n",
    "        adata, pca_kwargs, plot_scree,\n",
    "        X_PCA_HVG, adata.var[HIGHLY_VARIABLE]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def run_phate_using_g(\n",
    "    adata: AnnData,\n",
    "    g: Graph = None,\n",
    "    phate_kwargs: dict = PHATE_KWARGS,\n",
    "    g_kwargs: dict = G_KWARGS,\n",
    "    emb_key: str = X_PHATE,\n",
    ") -> Tuple[AnnData, Graph]:  \n",
    "    # Make initial cellwise graph with HVGS (auto t=46)\n",
    "    g_kwargs['random_state'] = 3\n",
    "    g_kwargs['n_pca'] = None\n",
    "    phate_kwargs['random_state'] = 3 \n",
    "\n",
    "    if g is None:        \n",
    "        pca_key = emb_key.replace(PHATE, PCA)        \n",
    "        print((\n",
    "            f'g is None. Will attempt to calculate with'\n",
    "            f' {PCA.upper()} stored in {ADATA}.obsm[{pca_key}].'\n",
    "        ))\n",
    "\n",
    "        if pca_key not in adata.obsm:\n",
    "            raise ValueError(f'{pca_key} not in {ADATA}.obsm')\n",
    "        \n",
    "        g = gt.Graph(adata.obsm[pca_key], **g_kwargs)\n",
    "\n",
    "    phate_op = phate.PHATE(**phate_kwargs)\n",
    "    data_phate = phate_op.fit_transform(g)\n",
    "    adata.obsm[emb_key] = data_phate\n",
    "    return adata, g\n",
    "\n",
    "def run_phate_on_hvg(\n",
    "    adata: AnnData,    \n",
    "    g: Graph = None,\n",
    "    phate_kwargs: dict = PHATE_KWARGS,\n",
    "    g_kwargs: dict = G_KWARGS,    \n",
    "    emb_key: str = X_PHATE_HVG,    \n",
    ") -> Tuple[AnnData, Graph]:\n",
    "    return run_phate_using_g(\n",
    "        adata, g, phate_kwargs, g_kwargs, emb_key\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_magic(\n",
    "    adata: AnnData, g: Graph, magic_kwargs: dict = MAGIC_KWARGS\n",
    ") -> AnnData:\n",
    "    G = copy.deepcopy(g)\n",
    "    knn_max = MAGIC_KWARGS.get('knn_max', 60)\n",
    "    G.knn_max = knn_max\n",
    "    G.data = adata.to_df()\n",
    "    G.data_nu = adata.to_df()\n",
    "    magic_op = magic.MAGIC(**magic_kwargs).fit(adata.to_df(), graph=G)\n",
    "    data_magic = magic_op.transform(genes='all_genes')\n",
    "    adata.layers[X_MAGIC] = scipy.sparse.csr_matrix(data_magic)\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
